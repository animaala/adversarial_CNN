{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Various constants for describing the data set\n",
    "\n",
    "# number of classes is 2 (go and stop)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Width and height of each image. (pixels)\n",
    "WIDTH = 72\n",
    "HEIGHT = 72\n",
    "\n",
    "# Number of channels in each image, 3 channels: Red, Green, Blue.\n",
    "NUM_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to read a single image from input file\n",
    "def get_image(filename, name=\"get_image\"):\n",
    "    with tf.name_scope(name):\n",
    "        # convert filename to a queue for an input pipeline.\n",
    "        filename_queue = tf.train.string_input_producer([filename], num_epochs=None)\n",
    "\n",
    "        # object to read records\n",
    "        reader = tf.TFRecordReader()\n",
    "\n",
    "        # read the full set of features for a single example\n",
    "        key, example = reader.read(filename_queue)\n",
    "\n",
    "        # parse the full example into its' component features.\n",
    "        features = tf.parse_single_example(\n",
    "            example,\n",
    "            features={\n",
    "                'image/height': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/width': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/colorspace': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
    "                'image/channels': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/class/label': tf.FixedLenFeature([], tf.int64),\n",
    "                'image/class/text': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
    "                'image/format': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
    "                'image/filename': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n",
    "                'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value='')\n",
    "            })\n",
    "\n",
    "        # now we are going to manipulate the label and image features\n",
    "\n",
    "        label = features['image/class/label']\n",
    "        image_buffer = features['image/encoded']\n",
    "\n",
    "        # Decode the jpeg\n",
    "        # name_scope effects ops\n",
    "        with tf.name_scope('decode_jpeg', [image_buffer], None):\n",
    "            # decode turns tensor of type string. 0-D the JPEG encoded image\n",
    "            # to tensor of type uint8. 3-D with shape [height, width, channels]\n",
    "            image = tf.image.decode_jpeg(image_buffer, channels=3, name=\"decode\")\n",
    "            image = tf.image.convert_image_dtype(image, dtype=tf.float32, name=\"convert_dtype\")\n",
    "\n",
    "        image.set_shape([HEIGHT, WIDTH, NUM_CHANNELS])\n",
    "\n",
    "        # re-define label as a \"one-hot\" vector\n",
    "        # it will be [0,1] or [1,0] here.\n",
    "        # This approach can easily be extended to more classes\n",
    "        label = tf.stack(tf.one_hot(label - 1, NUM_CLASSES), name=\"one_hot\")\n",
    "\n",
    "        return label, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"label\" and \"image\" are associated with corresponding feature from a single example in the training data file\n",
    "# at this point label is one hot vector. If label = 1 then [1,0]... if label = 2 then [0,1]\n",
    "# (and yes that's opposite to binary!)\n",
    "label, image = get_image(\"../../dataset/traffic_sign/train-00000-of-00001\")\n",
    "\n",
    "\n",
    "# and similarly for the validation data\n",
    "vlabel, vimage = get_image(\"../../dataset/traffic_sign/validation-00000-of-00001\")\n",
    "\n",
    "\n",
    "# associate \"label_batch\" and \"image_batch\" objects with a randomly selected batch of labels and images respectively\n",
    "# train.shuffle_batch creates batches by randomly shuffling tensors. Adds to the current graph:\n",
    "# 1: A shuffling queue into which tensors from the tensors arg are enqueued.\n",
    "# 2: A dequeue_many operation to create batches from the queue.\n",
    "# 3: A QueueRunner to QUEUE_RUNNER collection, to enqueue the tensors from tensors arg.\n",
    "with tf.name_scope(\"shuffle_batch\"):\n",
    "    imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "        [image, label],\n",
    "        batch_size=64,\n",
    "        capacity=220,\n",
    "        min_after_dequeue=60)\n",
    "\n",
    "    # and similarly for the validation data\n",
    "    vimageBatch, vlabelBatch = tf.train.shuffle_batch(\n",
    "        [vimage, vlabel],\n",
    "        batch_size=64,\n",
    "        capacity=220,\n",
    "        min_after_dequeue=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders for data we will populate later\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # input X: 72*72*3 pixel images, the first dimension (None) will index the images in the mini-batch\n",
    "    X = tf.placeholder(tf.float32, [None, HEIGHT, WIDTH, NUM_CHANNELS], name=\"images\")\n",
    "    # similarly, we have a placeholder for true outputs (obtained from labels)\n",
    "    Y_ = tf.placeholder(tf.float32, [None, NUM_CLASSES], name=\"labels\")\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    # Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\n",
    "    pkeep = tf.placeholder(tf.float32, name=\"dropout_prob\")\n",
    "    tf.summary.image(\"input\", X, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    \"\"\"\n",
    "    The convolutional model: 3 conv layers with kernel shape [filter_height, filter_width, in_channels, out_channels]\n",
    "    and 2 fully connected layers, one to bring all the activation maps together (outputs of all the filters) and one\n",
    "    final layer to predict a class\n",
    "    :return: The predictions Y and the logits\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"the_model\"):\n",
    "        # three convolutional layers with their channel counts, and a\n",
    "        # fully connected layer (the last layer has 2 softmax neurons for \"stop\" and \"go\")\n",
    "        I = 128  # 1st convolutional layer output channels\n",
    "        J = 128  # 2nd convolutional layer output channels\n",
    "        K = 160  # 3rd convolutional layer output channels\n",
    "        L = 256  # 4th\n",
    "        M = 384  # 5th\n",
    "        N = 2048 # fully connected layer\n",
    "\n",
    "        # weights / kernels\n",
    "        # 7x7 patch, 3 input channel, J output channels\n",
    "        W1 = tf.Variable(tf.truncated_normal([7, 7, NUM_CHANNELS, I], stddev=0.1))\n",
    "        W2 = tf.Variable(tf.truncated_normal([5, 5, I, J], stddev=0.1))\n",
    "        W3 = tf.Variable(tf.truncated_normal([3, 3, J, K], stddev=0.1))\n",
    "        W4 = tf.Variable(tf.truncated_normal([3, 3, K, L], stddev=0.1))\n",
    "        W5 = tf.Variable(tf.truncated_normal([3, 3, L, M]))\n",
    "        W6 = tf.Variable(tf.truncated_normal([5 * 5 * M, N], stddev=0.1))\n",
    "        W7 = tf.Variable(tf.truncated_normal([N, NUM_CLASSES], stddev=0.1))\n",
    "\n",
    "     #   visualize_kernel(W1)\n",
    "\n",
    "        # biases\n",
    "        B1 = tf.Variable(tf.constant(0.1, tf.float32, [I]))\n",
    "        B2 = tf.Variable(tf.constant(0.1, tf.float32, [J]))\n",
    "        B3 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "        B4 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "        B5 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "        B6 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "        B7 = tf.Variable(tf.constant(0.1, tf.float32, [NUM_CLASSES]))\n",
    "\n",
    "        # 72x72\n",
    "        X1 = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        tf.summary.image(\"MAX_POOL1\", X1, 4)\n",
    "        # 36x36\n",
    "        X2 = tf.nn.max_pool(X1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        tf.summary.image(\"MAX_POOL2\", X2, 4)\n",
    "        #18x18\n",
    "        X3 = tf.nn.max_pool(X2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        tf.summary.image(\"MAX_POOL3\", X3, 4)\n",
    "        #9x9\n",
    "        X4 = tf.nn.max_pool(X3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "        tf.summary.image(\"MAX_POOL4\", X4, 4)\n",
    "\n",
    "        with tf.name_scope(\"first_layer\"):\n",
    "            # 72x72 images\n",
    "            Y1r = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME') + B1)\n",
    "            # 36x36 images after max_pool\n",
    "            Y1p = tf.nn.max_pool(Y1r, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "            Y1 = tf.nn.dropout(Y1p, pkeep)\n",
    "\n",
    "        with tf.name_scope(\"second_layer\"):\n",
    "            Y2r = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, 1, 1, 1], padding='SAME') + B2)\n",
    "            # 18x18 images after max_pool\n",
    "            Y2p = tf.nn.max_pool(Y2r, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "            Y2 = tf.nn.dropout(Y2p, pkeep)\n",
    "\n",
    "        with tf.name_scope(\"third_layer\"):\n",
    "            Y3r = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, 1, 1, 1], padding='SAME') + B3)\n",
    "            # 9x9 images after max_pool\n",
    "            Y3p = tf.nn.max_pool(Y3r, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "            Y3 = tf.nn.dropout(Y3p, pkeep)\n",
    "\n",
    "        with tf.name_scope(\"fourth_layer\"):\n",
    "            Y4r = tf.nn.relu(tf.nn.conv2d(Y3, W4, strides=[1, 1, 1, 1], padding='SAME') + B4)\n",
    "            # 5x5 images after max_pool\n",
    "            Y4p = tf.nn.max_pool(Y4r, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "            Y4 = tf.nn.dropout(Y4p, pkeep)\n",
    "\n",
    "        with tf.name_scope(\"fifth_layer\"):\n",
    "            Y5r = tf.nn.relu(tf.nn.conv2d(Y4, W5, strides=[1, 1, 1, 1], padding='SAME') + B5)\n",
    "            Y5 = tf.nn.dropout(Y5r, pkeep)\n",
    "\n",
    "        with tf.name_scope(\"fc_layer\"):\n",
    "            YY = tf.reshape(Y5, shape=[-1, 5 * 5 * M])\n",
    "            Y6 = tf.nn.relu(tf.matmul(YY, W6) + B6)\n",
    "\n",
    "            YY6 = tf.nn.dropout(Y6, pkeep)\n",
    "            Ylogits = tf.matmul(YY6, W7) + B7\n",
    "            Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "        return Y, Ylogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y, Ylogits = model()\n",
    "\n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 50 images\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "with tf.name_scope(\"x-ent\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy) * 64\n",
    "    tf.summary.scalar(\"x-ent\", cross_entropy)\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# training step, the learning rate is a placeholder\n",
    "with tf.name_scope(\"train\"):\n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "# interactive session allows interleaving of building and running steps\n",
    "sess = tf.InteractiveSession()\n",
    "# init\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# start the threads used for reading files\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./traffic_graph/1.1\", sess.graph)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, training accuracy 0.484375\n",
      "step 200, training accuracy 0.4375\n",
      "step 300, training accuracy 0.375\n",
      "step 400, training accuracy 0.5\n",
      "step 500, training accuracy 0.5\n",
      "step 600, training accuracy 0.5625\n",
      "step 700, training accuracy 0.484375\n",
      "step 800, training accuracy 0.5\n",
      "step 900, training accuracy 0.5\n",
      "step 1000, training accuracy 0.53125\n",
      "step 1100, training accuracy 0.46875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ec0fa3bc901e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnSteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimageBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelBatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmerged_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.004\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpkeep\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "nSteps = 2000\n",
    "for i in range(nSteps):\n",
    "\n",
    "    batch_xs, batch_ys = sess.run([imageBatch, labelBatch])\n",
    "\n",
    "    s, k = sess.run([merged_summary, train_step], feed_dict={X: batch_xs, Y_: batch_ys, lr: 0.0007, pkeep: 0.5})\n",
    "    writer.add_summary(s, i)\n",
    "\n",
    "    if (i + 1) % 100 == 0:  # then perform validation\n",
    "\n",
    "        # get a validation batch\n",
    "        vbatch_xs, vbatch_ys = sess.run([vimageBatch, vlabelBatch])\n",
    "        train_accuracy = accuracy.eval(feed_dict={X: vbatch_xs, Y_: vbatch_ys, lr: 0.0007, pkeep: 1.0})\n",
    "\n",
    "        print(\"step %d, training accuracy %g\" % (i + 1, train_accuracy))\n",
    "\n",
    "\n",
    "# finalise\n",
    "coord.request_stop()\n",
    "coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
